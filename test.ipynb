{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#For ignoring warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: In this dataset, YES=2 & NO=1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for Duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Duplicates\n",
    "df=df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for Duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this dataset, GENDER & LUNG_CANCER attributes are in object data type. So, let's convert them to numerical values using LabelEncoder from sklearn.\n",
    "LabelEncoder is a utility class to help normalize labels such that they contain only values between 0 and n_classes-1.\n",
    "It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels. Also let's make every other attributes as YES=1 & NO=0.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T10:38:47.605213Z",
     "iopub.status.busy": "2023-03-23T10:38:47.604802Z",
     "iopub.status.idle": "2023-03-23T10:38:47.675051Z",
     "shell.execute_reply": "2023-03-23T10:38:47.673440Z",
     "shell.execute_reply.started": "2023-03-23T10:38:47.605171Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le=preprocessing.LabelEncoder()\n",
    "df['GENDER']=le.fit_transform(df['GENDER'])\n",
    "df['LUNG_CANCER']=le.fit_transform(df['LUNG_CANCER'])\n",
    "df['SMOKING']=le.fit_transform(df['SMOKING'])\n",
    "df['YELLOW_FINGERS']=le.fit_transform(df['YELLOW_FINGERS'])\n",
    "df['ANXIETY']=le.fit_transform(df['ANXIETY'])\n",
    "df['PEER_PRESSURE']=le.fit_transform(df['PEER_PRESSURE'])\n",
    "df['CHRONIC DISEASE']=le.fit_transform(df['CHRONIC DISEASE'])\n",
    "df['FATIGUE ']=le.fit_transform(df['FATIGUE '])\n",
    "df['ALLERGY ']=le.fit_transform(df['ALLERGY '])\n",
    "df['WHEEZING']=le.fit_transform(df['WHEEZING'])\n",
    "df['ALCOHOL CONSUMING']=le.fit_transform(df['ALCOHOL CONSUMING'])\n",
    "df['COUGHING']=le.fit_transform(df['COUGHING'])\n",
    "df['SHORTNESS OF BREATH']=le.fit_transform(df['SHORTNESS OF BREATH'])\n",
    "df['SWALLOWING DIFFICULTY']=le.fit_transform(df['SWALLOWING DIFFICULTY'])\n",
    "df['CHEST PAIN']=le.fit_transform(df['CHEST PAIN'])\n",
    "df['LUNG_CANCER']=le.fit_transform(df['LUNG_CANCER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le=preprocessing.LabelEncoder()\n",
    "def label_enc(col_list):\n",
    "    for i in col_list:\n",
    "        df[i]=le.fit_transform(df[i])\n",
    "col_list = df.columns.drop('AGE')\n",
    "label_enc(col_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check what's happened now\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Male=1 & Female=0. Also for other variables, YES=1 & NO=0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check the distributaion of Target variable.\n",
    "sns.countplot(x='LUNG_CANCER', data=df,)\n",
    "plt.title('Target Distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***That is, Target Distribution is imbalanced.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LUNG_CANCER'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***We will handle this imbalance before applyig algorithm.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's do some Data Visualizations for the better understanding of how the independent features are related to the target variable..**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting\n",
    "def plot(col, df=df):\n",
    "    return df.groupby(col)['LUNG_CANCER'].value_counts(normalize=True).unstack().plot(kind='bar', figsize=(8,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot('GENDER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot('AGE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot('SMOKING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot('YELLOW_FINGERS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot('ANXIETY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot('PEER_PRESSURE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot('CHRONIC DISEASE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot('FATIGUE ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot('ALLERGY ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot('WHEEZING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot('ALCOHOL CONSUMING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot('COUGHING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot('SHORTNESS OF BREATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot('SWALLOWING DIFFICULTY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot('CHEST PAIN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the visualizations, it is clear that in the given dataset, the features GENDER, AGE, SMOKING and SHORTNESS OF BREATH don't have that much relationship with LUNG CANCER. So let's drop those features to make this dataset more clean.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=df.drop(columns=['GENDER','AGE', 'SMOKING', 'SHORTNESS OF BREATH'])\n",
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CORRELATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding Correlation\n",
    "cn=df_new.corr()\n",
    "cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T10:38:52.458004Z",
     "iopub.status.busy": "2023-03-23T10:38:52.456711Z",
     "iopub.status.idle": "2023-03-23T10:38:53.595322Z",
     "shell.execute_reply": "2023-03-23T10:38:53.593566Z",
     "shell.execute_reply.started": "2023-03-23T10:38:52.457927Z"
    }
   },
   "outputs": [],
   "source": [
    "#Correlation \n",
    "cmap=sns.diverging_palette(260,-10,s=50, l=75, n=6,\n",
    "as_cmap=True)\n",
    "plt.subplots(figsize=(18,18))\n",
    "sns.heatmap(cn,cmap=cmap,annot=True, square=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kot = cn[cn>=.40]\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(kot, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Feature Engineering***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering is the process of creating new features using existing features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The correlation matrix shows that ANXIETY and YELLOW_FINGERS are correlated more than 50%. So, lets create a new feature combining them.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['ANXYELFIN']=df_new['ANXIETY']*df_new['YELLOW_FINGERS']\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting independent and dependent variables\n",
    "X = df_new.drop('LUNG_CANCER', axis = 1)\n",
    "y = df_new['LUNG_CANCER']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Target Distribution Imbalance Handling***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X, y = adasyn.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data for training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size= 0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting training data to the model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_model=LogisticRegression(random_state=0)\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting result using testing data\n",
    "y_lr_pred= lr_model.predict(X_test)\n",
    "y_lr_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "lr_cr=classification_report(y_test, y_lr_pred)\n",
    "print(lr_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This model is almost 97% accurate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting training data to the model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_model= DecisionTreeClassifier(criterion='entropy', random_state=0)  \n",
    "dt_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting result using testing data\n",
    "y_dt_pred= dt_model.predict(X_test)\n",
    "y_dt_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy\n",
    "dt_cr=classification_report(y_test, y_dt_pred)\n",
    "print(dt_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This model is 94% accurate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **K Nearest Neighbor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting K-NN classifier to the training set  \n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "knn_model= KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2 )  \n",
    "knn_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting result using testing data\n",
    "y_knn_pred= knn_model.predict(X_test)\n",
    "y_knn_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy\n",
    "knn_cr=classification_report(y_test, y_knn_pred)\n",
    "print(knn_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This model is 96% accurate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Gaussian Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting Gaussian Naive Bayes classifier to the training set  \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting result using testing data\n",
    "y_gnb_pred= gnb_model.predict(X_test)\n",
    "y_gnb_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy\n",
    "gnb_cr=classification_report(y_test, y_gnb_pred)\n",
    "print(gnb_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This model is 92% accurate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Multinomial Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting Multinomial Naive Bayes classifier to the training set  \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb_model = MultinomialNB()\n",
    "mnb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting result using testing data\n",
    "y_mnb_pred= mnb_model.predict(X_test)\n",
    "y_mnb_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy\n",
    "mnb_cr=classification_report(y_test, y_mnb_pred)\n",
    "print(mnb_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This model is 81% accurate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Support Vector Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting SVC to the training set  \n",
    "from sklearn.svm import SVC\n",
    "svc_model = SVC()\n",
    "svc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting result using testing data\n",
    "y_svc_pred= svc_model.predict(X_test)\n",
    "y_svc_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy\n",
    "svc_cr=classification_report(y_test, y_svc_pred)\n",
    "print(svc_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This model is 98% accurate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting result using testing data\n",
    "y_rf_pred= rf_model.predict(X_test)\n",
    "y_rf_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy\n",
    "rf_cr=classification_report(y_test, y_rf_pred)\n",
    "print(rf_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This model is also 98% accurate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting result using testing data\n",
    "y_xgb_pred= xgb_model.predict(X_test)\n",
    "y_xgb_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy\n",
    "xgb_cr=classification_report(y_test, y_xgb_pred)\n",
    "print(xgb_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This model is also 97% accurate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T06:18:38.849499Z",
     "iopub.status.busy": "2023-02-24T06:18:38.849101Z",
     "iopub.status.idle": "2023-02-24T06:18:38.854549Z",
     "shell.execute_reply": "2023-02-24T06:18:38.853244Z",
     "shell.execute_reply.started": "2023-02-24T06:18:38.849467Z"
    }
   },
   "source": [
    "# **Multi-layer Perceptron classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training a neural network model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp_model = MLPClassifier()\n",
    "mlp_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting result using testing data\n",
    "y_mlp_pred= mlp_model.predict(X_test)\n",
    "y_mlp_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy\n",
    "mlp_cr=classification_report(y_test, y_mlp_pred)\n",
    "print(mlp_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This model is also 98% accurate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T06:21:03.647586Z",
     "iopub.status.busy": "2023-02-24T06:21:03.647154Z",
     "iopub.status.idle": "2023-02-24T06:21:03.653955Z",
     "shell.execute_reply": "2023-02-24T06:21:03.652238Z",
     "shell.execute_reply.started": "2023-02-24T06:21:03.647552Z"
    }
   },
   "source": [
    "# **Gradient Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_model = GradientBoostingClassifier()\n",
    "gb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting result using testing data\n",
    "y_gb_pred= gb_model.predict(X_test)\n",
    "y_gb_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy\n",
    "gb_cr=classification_report(y_test, y_gb_pred)\n",
    "print(gb_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This model is also 98% accurate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above calculated accuracies, it is clear that the SVC, Random Forest, Multi-layer Perceptron and Gradient Boost models performed atmost level while the worst performed one is Multinomial Naive Bayes. \n",
    "However, I'm interested in a more efficient way of evaluating these models. Let's go for the Cross Validation methods using both K-Fold and Stratified K-Fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cross Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Fold cross validation is a popular technique used in machine learning for model evaluation and selection. It involves dividing a dataset into K subsets of equal size, called folds. The algorithm then trains and evaluates the model K times, each time using a different fold as the validation set and the remaining K-1 folds as the training set.\n",
    "\n",
    "During each iteration of K-Fold cross validation, the model is trained on K-1 folds and evaluated on the remaining fold. The performance metrics are then averaged over all K iterations to obtain an estimate of the model's overall performance.\n",
    "\n",
    "K-Fold cross validation is a robust method for model evaluation because it uses all the available data for training and testing. It also helps to reduce the risk of overfitting and provides a more accurate estimate of the model's performance than using a single training-test split.\n",
    "\n",
    "Typically, values of K between 5 and 10 are used for K-Fold cross validation, but the optimal value of K may vary depending on the size and complexity of the dataset, as well as the type of model being evaluated.\n",
    "\n",
    "Here are some general guidelines that can help you choose an appropriate value of K:\n",
    "\n",
    "1. For small datasets, it is recommended to use a larger value of K, such as 10. This is because the larger value of K allows for more robust estimates of model performance, given the limited amount of data.\n",
    "\n",
    "2. For larger datasets, a smaller value of K can be used, such as 5. This is because a larger value of K will result in K smaller training sets, which may not be representative of the full dataset. Using a smaller value of K ensures that each fold has a sufficient amount of data for both training and testing.\n",
    "\n",
    "3. For models that are computationally expensive or time-consuming to train, a smaller value of K is preferred to reduce the overall training time.\n",
    "\n",
    "4. It's also essential to note that the choice of K should not be based solely on the accuracy of the model. Other metrics, such as precision, recall, and F1 score, should also be considered, as they can provide valuable insights into the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "k = 10\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# Logistic regerssion model\n",
    "lr_model_scores = cross_val_score(lr_model,X, y, cv=kf)\n",
    "\n",
    "# Decision tree model\n",
    "dt_model_scores = cross_val_score(dt_model,X, y, cv=kf)\n",
    "\n",
    "# KNN model\n",
    "knn_model_scores = cross_val_score(knn_model,X, y, cv=kf)\n",
    "\n",
    "# Gaussian naive bayes model\n",
    "gnb_model_scores = cross_val_score(gnb_model,X, y, cv=kf)\n",
    "\n",
    "# Multinomial naive bayes model\n",
    "mnb_model_scores = cross_val_score(mnb_model,X, y, cv=kf)\n",
    "\n",
    "# Support Vector Classifier model\n",
    "svc_model_scores = cross_val_score(svc_model,X, y, cv=kf)\n",
    "\n",
    "# Random forest model\n",
    "rf_model_scores = cross_val_score(rf_model,X, y, cv=kf)\n",
    "\n",
    "# XGBoost model\n",
    "xgb_model_scores = cross_val_score(xgb_model,X, y, cv=kf)\n",
    "\n",
    "# Multi-layer perceptron model\n",
    "mlp_model_scores = cross_val_score(mlp_model,X, y, cv=kf)\n",
    "\n",
    "# Gradient boost model\n",
    "gb_model_scores = cross_val_score(gb_model,X, y, cv=kf)\n",
    "\n",
    "\n",
    "print(\"Logistic regression models' average accuracy:\", np.mean(lr_model_scores))\n",
    "print(\"Decision tree models' average accuracy:\", np.mean(dt_model_scores))\n",
    "print(\"KNN models' average accuracy:\", np.mean(knn_model_scores))\n",
    "print(\"Gaussian naive bayes models' average accuracy:\", np.mean(gnb_model_scores))\n",
    "print(\"Multinomial naive bayes models' average accuracy:\", np.mean(mnb_model_scores))\n",
    "print(\"Support Vector Classifier models' average accuracy:\", np.mean(svc_model_scores))\n",
    "print(\"Random forest models' average accuracy:\", np.mean(rf_model_scores))\n",
    "print(\"XGBoost models' average accuracy:\", np.mean(xgb_model_scores))\n",
    "print(\"Multi-layer perceptron models' average accuracy:\", np.mean(mlp_model_scores))\n",
    "print(\"Gradient boost models' average accuracy:\", np.mean(gb_model_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So the K-Fold cross validation is showing Gradient Boost model gives the most accuracy of 95.6%, and Decision Tree model also gives almost same accuracy, while Multinomial Naive Bayes model gives the least accuarcy of 74.2%.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratified K-Fold cross-validation is a modification of the standard K-Fold cross-validation technique that is commonly used in machine learning when working with imbalanced datasets. The goal of Stratified K-Fold cross-validation is to ensure that each fold is representative of the overall dataset in terms of the class distribution.\n",
    "\n",
    "In standard K-Fold cross-validation, the data is split into K folds, and each fold is used as the validation set in turn. However, if the dataset has an imbalanced class distribution, this can lead to some of the folds having significantly fewer samples from the minority class, which can result in biased performance estimates.\n",
    "\n",
    "To address this issue, Stratified K-Fold cross-validation ensures that each fold has a similar proportion of samples from each class. It works by first dividing the dataset into K folds, as in standard K-Fold cross-validation. Then, for each fold, the algorithm ensures that the proportion of samples from each class is roughly the same as the proportion in the full dataset. This ensures that the model is evaluated on a representative sample of the data, regardless of the class distribution.\n",
    "\n",
    "Stratified K-Fold cross-validation is a powerful tool for evaluating the performance of machine learning models on imbalanced datasets. It can help to ensure that the model's performance is accurately estimated and that the model is robust to class imbalances in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "k = 10\n",
    "kf = StratifiedKFold(n_splits=k)\n",
    "\n",
    "\n",
    "# Logistic regerssion model\n",
    "lr_model_scores = cross_val_score(lr_model,X, y, cv=kf)\n",
    "\n",
    "# Decision tree model\n",
    "dt_model_scores = cross_val_score(dt_model,X, y, cv=kf)\n",
    "\n",
    "# KNN model\n",
    "knn_model_scores = cross_val_score(knn_model,X, y, cv=kf)\n",
    "\n",
    "# Gaussian naive bayes model\n",
    "gnb_model_scores = cross_val_score(gnb_model,X, y, cv=kf)\n",
    "\n",
    "# Multinomial naive bayes model\n",
    "mnb_model_scores = cross_val_score(mnb_model,X, y, cv=kf)\n",
    "\n",
    "# Support Vector Classifier model\n",
    "svc_model_scores = cross_val_score(svc_model,X, y, cv=kf)\n",
    "\n",
    "# Random forest model\n",
    "rf_model_scores = cross_val_score(rf_model,X, y, cv=kf)\n",
    "\n",
    "# XGBoost model\n",
    "xgb_model_scores = cross_val_score(xgb_model,X, y, cv=kf)\n",
    "\n",
    "# Multi-layer perceptron model\n",
    "mlp_model_scores = cross_val_score(mlp_model,X, y, cv=kf)\n",
    "\n",
    "# Gradient boost model\n",
    "gb_model_scores = cross_val_score(gb_model,X, y, cv=kf)\n",
    "\n",
    "\n",
    "print(\"Logistic regression models' average accuracy:\", np.mean(lr_model_scores))\n",
    "print(\"Decision tree models' average accuracy:\", np.mean(dt_model_scores))\n",
    "print(\"KNN models' average accuracy:\", np.mean(knn_model_scores))\n",
    "print(\"Gaussian naive bayes models' average accuracy:\", np.mean(gnb_model_scores))\n",
    "print(\"Multinomial naive bayes models' average accuracy:\", np.mean(mnb_model_scores))\n",
    "print(\"Support Vector Classifier models' average accuracy:\", np.mean(svc_model_scores))\n",
    "print(\"Random forest models' average accuracy:\", np.mean(rf_model_scores))\n",
    "print(\"XGBoost models' average accuracy:\", np.mean(xgb_model_scores))\n",
    "print(\"Multi-layer perceptron models' average accuracy:\", np.mean(mlp_model_scores))\n",
    "print(\"Gradient boost models' average accuracy:\", np.mean(gb_model_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So the Stratified K-Fold cross validation is showing Random Forest model gives the most accuracy of 94.6%, and also other models like Gradient Boost, Support Vector Classifier, XGBoost gives almost same accuracies, while Multinomial Naive Bayes model gives the least accuarcy of 75.7%.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
